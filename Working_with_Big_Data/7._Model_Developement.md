# Model Development


## What is Model Development?
Model development involves creating mathematical representations, or models, that can predict certain outcomes based on input data. 


## Why is it Important?
Model development is crucial because it allows us to make informed decisions and predictions. By understanding the relationships between different variables.


## Steps in Model Development


1. **Data Collection**: Gather relevant data that includes both input variables (features) and the target variable (outcome to predict).

2. **Data Preprocessing**: Clean the data by handling missing values, removing outliers, and transforming variables if needed.

3. **Model Selection**: Choose the appropriate type of model based on the nature of the data and the prediction task.

4. **Model Training**: Train the selected model using the prepared dataset to learn the patterns and relationships between input and output variables.

5. **Model Evaluation**: Assess the performance of the trained model using evaluation metrics to determine how well it predicts outcomes.

6. **Model Deployment**: Deploy the trained model for practical use, making predictions on new data as needed.

7. **Model Monitoring and Maintenance**: Continuously monitor the model's performance over time, retrain it with new data if necessary, and update it to ensure accurate predictions.


# Linear Regression
Linear regression analysis is used to predict the value of a variable based on the value of another variable. The variable you want to predict is called the dependent variable. The variable you are using to predict the other variable's value is called the independent variable.

## Types of Linear Regression

There are two kinds of Linear Regression Model:

**Simple Linear Regression**:

Simple linear regression is used to model the relationship between a single independent variable (X) and a dependent variable (y). The model assumes that this relationship is linear, meaning that a change in X is associated with a constant change in y.

```python
import numpy as np
from sklearn.linear_model import LinearRegression

 # Example data
X_simple = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Reshape to a column vector
y_simple = np.array([2, 4, 5, 4, 5])

 # Create a linear regression model
simple_reg = LinearRegression()

 # Fit the model to the data
simple_reg.fit(X_simple, y_simple)

 # Print the slope (coefficient) and intercept of the line
print("Simple Linear Regression Coefficient (slope):", simple_reg.coef_[0])
print("Simple Linear Regression Intercept:", simple_reg.intercept_)
```

**Multiple Linear Regression**:

Multiple linear regression extends the concept of simple linear regression to multiple independent variables. It models the relationship between multiple independent variables (X1, X2, ..., Xn) and a dependent variable (y) as a linear combination of the independent variables.

```python
import numpy as np
from sklearn.linear_model import LinearRegression

 # Example data
X_multiple = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])  # Two independent variables
y_multiple = np.array([2, 4, 5, 4, 5])

 # Create a linear regression model
multiple_reg = LinearRegression()

 # Fit the model to the data
multiple_reg.fit(X_multiple, y_multiple)

 # Print the coefficients (slope) and intercept of the line
print("Multiple Linear Regression Coefficients (slopes):", multiple_reg.coef_)
print("Multiple Linear Regression Intercept:", multiple_reg.intercept_)
```

## Assumptions of Linear Regression
Before attempting to perform linear regression, it's essential to ensure that your data meets certain required assumptions. Linear regression models rely on these assumptions to provide accurate and reliable results. Let's outline each assumption and its significance in the context of linear regression:

1. **Linearity:** 

The relationship between the independent variables (features) and the dependent variable (target) should be linear. This means that the relationship between the things you're trying to predict (like house prices) and the factors you're using to make predictions (like square footage or number of bedrooms) should be a straight line. In simple terms, when one thing goes up, the other thing goes up or down consistently.


2. **No multicollinearity:**

This assumption applies when there are multiple independent variables in the model. It states that the independent variables should not be highly correlated with each other. Multicollinearity can make it difficult to determine the effect of each individual independent variable on the target variable.


3. **Normality of residuals:** 

The residuals should follow a normal distribution. This means that the errors (residuals) should be symmetrically distributed around zero. Deviations from normality may indicate that the model is not capturing all the underlying patterns in the data.


4. **Homoscedasticity:** 

Also known as constant variance, it means that the variance of the residuals (the differences between the observed and predicted values) should be constant across all levels of the independent variables. This assumption implies that the spread of the residuals should be consistent as you move along the regression line.


5. **No autocorrelation:**

Autocorrelation occurs when the residuals are correlated with each other. In other words, the error terms for one observation should not be dependent on the error terms of another observation. Autocorrelation can arise in time series data or spatial data, for example.â€ƒ

## Code Example

```python
 # Importing necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import os
import matplotlib.pyplot as plt
%matplotlib inline  # Magic command to display plots inline in Jupyter Notebook
from sklearn.linear_model import LinearRegression  # Importing LinearRegression model

 # Reading the dataset
df = pd.read_csv(r"data.csv")

 # Extracting independent variables (features) and dependent variable (target) from the dataset
X = df.iloc[:,0:3].values  # Features
Y = df.iloc[:,-1].values   # Target

 # Splitting the dataset into training and testing sets
from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3,random_state=1)

 # Creating and fitting the Linear Regression model
model = LinearRegression()
model.fit(X_train,Y_train)

 # Making predictions on the test set
Y_pred = model.predict(X_test)

 # Calculating residuals (errors)
residual = Y_test - Y_pred

 ## 1. Linearity
 # Visualizing the relationship between features and target variable
fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(12,2.5))  # Creating subplots
ax1.scatter(df["feature1"], df["target"])  # Scatter plot for Feature 1 vs Target
ax1.set_title("Feature-1")  # Setting title for subplot 1
ax2.scatter(df["feature2"], df["target"])  # Scatter plot for Feature 2 vs Target
ax2.set_title("Feature-2")  # Setting title for subplot 2
ax3.scatter(df["feature3"], df["target"])  # Scatter plot for Feature 3 vs Target
ax3.set_title("Feature-3")  # Setting title for subplot 3
plt.show()  # Displaying the plot

 ## 2. No multicollinearity
 # Calculating and visualizing Variance Inflation Factor (VIF)
from statsmodels.stats.outliers_influence import variance_inflation_factor
vif = []

for i in range(X_train.shape[1]):  # Looping through each feature
    vif.append(variance_inflation_factor(X_train, i))  # Calculating VIF for each feature

 # Creating DataFrame to display VIF values
pd.DataFrame({"vif":vif}, index=df.columns[0:3]).T

 # Visualizing correlation matrix using heatmap
sns.heatmap(df.iloc[:,0:3].corr(), annot=True)  # Plotting heatmap of correlations between features
plt.show()  # Displaying the plot

 ## 3. Normality of residuals
 # Visualizing the distribution of residuals using Kernel Density Estimate (KDE) plot
sns.displot(residual, kind="kde")  # Plotting KDE plot of residuals
import scipy as sp
 # Creating Probability Plot to check normality of residuals
fig, ax = plt.subplots(figsize =(6,4))  # Creating a plot
sp.stats.probplot(residual,plot=ax, fit=True)  # Creating probability plot
plt.show()  # Displaying the plot

 ## 4. Homoscedasticity
 # Visualizing homoscedasticity by plotting residuals against predicted values
plt.scatter(Y_pred, residual)  # Scatter plot of predicted values vs residuals
plt.xlabel("Predicted Values (Y)")  # Setting x-axis label
plt.ylabel("Residual (Errors)")  # Setting y-axis label
residual.mean()  # Calculating mean of residuals

 ## 5. No autocorrelation
 # Visualizing residuals to check for autocorrelation
plt.plot(residual)  # Plotting residuals

```

# Evaluation of Regression Model Using Metrics

## Mean Absolute Error (MAE)

**Definition**: MAE is the average of the absolute differences between predicted and actual values.

**Code Example**:
```python
from sklearn.metrics import mean_absolute_error

y_true = [10, 20, 30, 40]
y_pred = [12, 18, 28, 38]

mae = mean_absolute_error(y_true, y_pred)
print("MAE:", mae)
```

**Real-life Example**: Imagine predicting delivery times for packages. If your MAE is 2 minutes, it means, on average, your predictions are off by 2 minutes.

---

## Mean Squared Error (MSE)

**Definition**: MSE is the average of the squared differences between predicted and actual values.

**Code Example**:
```python
from sklearn.metrics import mean_squared_error

mse = mean_squared_error(y_true, y_pred)
print("MSE:", mse)
```

**Real-life Example**: If your MSE is 20, it means, on average, the squared difference between your predicted and actual delivery times is 20 minutes squared.

---

## Root Mean Squared Error (RMSE)

**Definition**: RMSE is the square root of the MSE.

**Code Example**:
```python
import numpy as np

rmse = np.sqrt(mse)
print("RMSE:", rmse)
```

**Real-life Example**: An RMSE of 4 means, on average, your predictions are off by 4 minutes, in the same unit as your delivery times.

---

## R-squared

**Definition**: R-squared is the proportion of the variance in the dependent variable that is predictable from the independent variables.

**Code Example**:
```python
from sklearn.metrics import r2_score

r2 = r2_score(y_true, y_pred)
print("R-squared:", r2)
```

**Real-life Example**: An R-squared of 0.85 means your model explains 85% of the variability in delivery times, and 15% is not explained by your model.

---

## Adjusted R-squared

**Definition**: Adjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in the model.

**Real-life Example**: Adjusted R-squared penalizes the addition of unnecessary variables that do not improve the model significantly, providing a more accurate representation of the model's goodness of fit in predicting delivery times.

---

# Polynomial Regression: Modeling Non-Linear Relationships

Polynomial regression is a form of regression analysis used to model the relationship between variables when the relationship is not linear. This technique allows for more flexibility in capturing complex relationships between the independent and dependent variables. The polynomial regression model is defined by an \( n \)th degree polynomial equation, where \( n \) is the degree of the polynomial.

## Polynomial Regression

To perform polynomial regression in Python, we can use the `PolynomialFeatures` class from `sklearn.preprocessing` to transform the original features into polynomial features up to the desired degree. We then fit a linear regression model to these polynomial features to capture the non-linear relationship.

Here's an example demonstrating how to perform polynomial regression and visualize the results using Python:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

 # Generating some random data
np.random.seed(0)
X = 2 * np.random.rand(100, 1) - 1
y = 0.5 * X**2 + X + 2 + np.random.randn(100, 1)

 # Fitting polynomial regression
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X)

lin_reg = LinearRegression()
lin_reg.fit(X_poly, y)

 # Plotting the data and the polynomial regression line
plt.scatter(X, y, color='blue', label='Actual data')
plt.plot(np.sort(X, axis=0), lin_reg.predict(poly_features.transform(np.sort(X, axis=0))), color='red', label='Polynomial regression')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Polynomial Regression Example')
plt.legend()
plt.show()
```



