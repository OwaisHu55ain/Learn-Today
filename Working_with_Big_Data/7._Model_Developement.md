# Model Development


## What is Model Development?
Model development involves creating mathematical representations, or models, that can predict certain outcomes based on input data. 


## Why is it Important?
Model development is crucial because it allows us to make informed decisions and predictions. By understanding the relationships between different variables.


## Steps in Model Development


1. **Data Collection**: Gather relevant data that includes both input variables (features) and the target variable (outcome to predict).

2. **Data Preprocessing**: Clean the data by handling missing values, removing outliers, and transforming variables if needed.

3. **Model Selection**: Choose the appropriate type of model based on the nature of the data and the prediction task.

4. **Model Training**: Train the selected model using the prepared dataset to learn the patterns and relationships between input and output variables.

5. **Model Evaluation**: Assess the performance of the trained model using evaluation metrics to determine how well it predicts outcomes.

6. **Model Deployment**: Deploy the trained model for practical use, making predictions on new data as needed.

7. **Model Monitoring and Maintenance**: Continuously monitor the model's performance over time, retrain it with new data if necessary, and update it to ensure accurate predictions.


## Linear Regression
Linear regression analysis is used to predict the value of a variable based on the value of another variable. The variable you want to predict is called the dependent variable. The variable you are using to predict the other variable's value is called the independent variable.


## Assumptions of Linear Regression
Before attempting to perform linear regression, it's essential to ensure that your data meets certain required assumptions. Linear regression models rely on these assumptions to provide accurate and reliable results. Let's outline each assumption and its significance in the context of linear regression:

1. Linearity: 
The relationship between the independent variables (features) and the dependent variable (target) should be linear. This means that the relationship between the things you're trying to predict (like house prices) and the factors you're using to make predictions (like square footage or number of bedrooms) should be a straight line. In simple terms, when one thing goes up, the other thing goes up or down consistently.


2. No multicollinearity: 
This assumption applies when there are multiple independent variables in the model. It states that the independent variables should not be highly correlated with each other. Multicollinearity can make it difficult to determine the effect of each individual independent variable on the target variable.


3. Normality of residuals: 
The residuals should follow a normal distribution. This means that the errors (residuals) should be symmetrically distributed around zero. Deviations from normality may indicate that the model is not capturing all the underlying patterns in the data.


4. Homoscedasticity: 
Also known as constant variance, it means that the variance of the residuals (the differences between the observed and predicted values) should be constant across all levels of the independent variables. This assumption implies that the spread of the residuals should be consistent as you move along the regression line.


5. No autocorrelation: 
Autocorrelation occurs when the residuals are correlated with each other. In other words, the error terms for one observation should not be dependent on the error terms of another observation. Autocorrelation can arise in time series data or spatial data, for example.â€ƒ


```python
 # Importing necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import os
import matplotlib.pyplot as plt
%matplotlib inline  # Magic command to display plots inline in Jupyter Notebook
from sklearn.linear_model import LinearRegression  # Importing LinearRegression model

 # Reading the dataset
df = pd.read_csv(r"data.csv")

 # Extracting independent variables (features) and dependent variable (target) from the dataset
X = df.iloc[:,0:3].values  # Features
Y = df.iloc[:,-1].values   # Target

 # Splitting the dataset into training and testing sets
from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3,random_state=1)

 # Creating and fitting the Linear Regression model
model = LinearRegression()
model.fit(X_train,Y_train)

 # Making predictions on the test set
Y_pred = model.predict(X_test)

 # Calculating residuals (errors)
residual = Y_test - Y_pred

 ## 1. Linearity
 # Visualizing the relationship between features and target variable
fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(12,2.5))  # Creating subplots
ax1.scatter(df["feature1"], df["target"])  # Scatter plot for Feature 1 vs Target
ax1.set_title("Feature-1")  # Setting title for subplot 1
ax2.scatter(df["feature2"], df["target"])  # Scatter plot for Feature 2 vs Target
ax2.set_title("Feature-2")  # Setting title for subplot 2
ax3.scatter(df["feature3"], df["target"])  # Scatter plot for Feature 3 vs Target
ax3.set_title("Feature-3")  # Setting title for subplot 3
plt.show()  # Displaying the plot

 ## 2. No multicollinearity
 # Calculating and visualizing Variance Inflation Factor (VIF)
from statsmodels.stats.outliers_influence import variance_inflation_factor
vif = []

for i in range(X_train.shape[1]):  # Looping through each feature
    vif.append(variance_inflation_factor(X_train, i))  # Calculating VIF for each feature

 # Creating DataFrame to display VIF values
pd.DataFrame({"vif":vif}, index=df.columns[0:3]).T

 # Visualizing correlation matrix using heatmap
sns.heatmap(df.iloc[:,0:3].corr(), annot=True)  # Plotting heatmap of correlations between features
plt.show()  # Displaying the plot

 ## 3. Normality of residuals
 # Visualizing the distribution of residuals using Kernel Density Estimate (KDE) plot
sns.displot(residual, kind="kde")  # Plotting KDE plot of residuals
import scipy as sp
 # Creating Probability Plot to check normality of residuals
fig, ax = plt.subplots(figsize =(6,4))  # Creating a plot
sp.stats.probplot(residual,plot=ax, fit=True)  # Creating probability plot
plt.show()  # Displaying the plot

 ## 4. Homoscedasticity
 # Visualizing homoscedasticity by plotting residuals against predicted values
plt.scatter(Y_pred, residual)  # Scatter plot of predicted values vs residuals
plt.xlabel("Predicted Values (Y)")  # Setting x-axis label
plt.ylabel("Residual (Errors)")  # Setting y-axis label
residual.mean()  # Calculating mean of residuals

 ## 5. No autocorrelation
 # Visualizing residuals to check for autocorrelation
plt.plot(residual)  # Plotting residuals

```
